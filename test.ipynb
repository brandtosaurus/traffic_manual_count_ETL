{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import config\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import calcs\n",
    "import config\n",
    "import zipfile\n",
    "\n",
    "import main\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "DROP_IF = [\"DO NOT FILL IN\", \"DO NOT F\"]\n",
    "\n",
    "\n",
    "# ENGINE = create_engine(\n",
    "#     r\"postgresql://postgres:$admin@localhost:5432/asset_management_master\"\n",
    "# )\n",
    "\n",
    "# ENGINE = create_engine(\n",
    "#     r\"postgresql://postgres:$admin@localhost:5432/asset_management_master\"\n",
    "# )\n",
    "\n",
    "TYPE = \"Detailed Manual Traffic Count Form\"\n",
    "\n",
    "DROP_IF = [\"DO NOT FILL IN\", \"DO NOT F\"]\n",
    "\n",
    "PATH = r\"C:\\Users\\MB2705851\\Desktop\\Temp\\manual_traffic_counts\\2022-04-06\"\n",
    "PATH2 = r\"C:\\Users\\MB2705851\\OneDrive - Surbana Jurong Private Limited\\Manuals & Guidelines\\Traffic\\Manual count import templates\\Flat Template.xlsx\"\n",
    "PATH3 = r\"C:\\Users\\MB2705851\\Desktop\\Temp\\manual_traffic_counts\\TEST\\216A-190905.xlsx\"\n",
    "OUTPATH = r\"C:\\Users\\MB2705851\\Desktop\\Temp\\Temp Excel\\TEST.csv\"\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_zip(path: str) -> bool:\n",
    "    for filename in path:\n",
    "        return zipfile.is_zipfile(filename)\n",
    "\n",
    "\n",
    "def getfiles(path) -> List[str]:\n",
    "    print(\"COLLECTING FILES......\")\n",
    "    src = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if name.endswith(\".xlsx\") or name.endswith(\".csv\"):\n",
    "                p = os.path.join(root, name)\n",
    "                src.append(p)\n",
    "    src = list(set(src))\n",
    "    return src\n",
    "\n",
    "\n",
    "def to_df(file: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file, header=None, sep=\"\\n\")\n",
    "    df = df[0].str.split(\"\\s+|,\\s+|,\", expand=True)\n",
    "    df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def join(header: pd.DataFrame, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    if data.empty:\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        q = \"\"\"\n",
    "\t\tSELECT header.header_id, header.station_name, data.*\n",
    "\t\tFROM header\n",
    "\t\tLEFT JOIN data ON data.start_datetime WHERE data.start_datetime >= header.start_datetime AND data.end_datetime <= header.end_datetime;\n",
    "\t\t\"\"\"\n",
    "        q2 = \"\"\"UPDATE data set header_id = (SELECT header_id from header WHERE data.start_datetime >= header.start_datetime AND data.counttime_end <= header.enddate)\"\"\"\n",
    "        pysqldf = lambda q: sqldf(q, globals())\n",
    "        df = sqldf(q, locals())\n",
    "        df = pd.DataFrame(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def data_join(data: pd.DataFrame, header: pd.DataFrame) -> pd.DataFrame:\n",
    "    if data is None:\n",
    "        pass\n",
    "    elif data.empty:\n",
    "        pass\n",
    "    else:\n",
    "        data = pd.DataFrame(data)\n",
    "        data = join(header, data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_to_temp_csv(df: pd.DataFrame, label: str):\n",
    "    if not os.path.exists(os.path.expanduser(config.OUTPUT_FILE + label + \".csv\")):\n",
    "        df.to_csv(\n",
    "            os.path.expanduser(config.OUTPUT_FILE + label + \".csv\"),\n",
    "            mode=\"a\",\n",
    "            header=True,\n",
    "            index=False,\n",
    "        )\n",
    "    else:\n",
    "        df.to_csv(\n",
    "            os.path.expanduser(config.OUTPUT_FILE + label + \".csv\"),\n",
    "            mode=\"a\",\n",
    "            header=False,\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def push_to_db(df: pd.DataFrame, table: str, subset: List[str]) -> None:\n",
    "    try:\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "    except Exception:\n",
    "        df = df.drop_duplicates(subset=subset)\n",
    "        df.to_sql(\n",
    "            table,\n",
    "            con=config.ENGINE,\n",
    "            schema=\"trafc\",\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            method=psql_insert_copy,\n",
    "        )\n",
    "\n",
    "\n",
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = \", \".join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = \"{}.{}\".format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = \"COPY {} ({}) FROM STDIN WITH CSV\".format(table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"C:\\FTP\\Trafftrans\\2022-06-24\\April\\1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLLECTING FILES......\n"
     ]
    }
   ],
   "source": [
    "files = getfiles(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\197.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\225.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\600.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\585.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\583.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\112.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\584.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\247.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\1590.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\1589.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\582.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\246.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\224.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\588.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\575.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\1594.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\673.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\1591.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\580.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\1588.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\642.xlsx',\n",
       " 'C:\\\\FTP\\\\Trafftrans\\\\2022-06-24\\\\April\\\\1\\\\302.xlsx']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completed_files():\n",
    "    completed_list = []\n",
    "    with open(r\"C:\\Users\\MB2705851\\Desktop\\Temp\\manual_traffic_counts\\COMPLETED_FILES.csv\", \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            completed_list.append(row[0])\n",
    "    return completed_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_etl(df, file) -> pd.DataFrame:\n",
    "\n",
    "    ## UNCOMMENT THIS WHEN WORKING WITH FILES\n",
    "    # xls = pd.ExcelFile(file)\n",
    "    # df = pd.read_excel(file, sheet_name=xls.sheet_names, header=None)\n",
    "    # for key, df in df.items():\n",
    "    \n",
    "    header = {\n",
    "        \"header_id\": [str(uuid.uuid4())],\n",
    "        \"document_url\": file,\n",
    "        \"counted_by\": [\"CKDM\"],\n",
    "        \"tc_station_name\": [df.loc[0, 1]],\n",
    "        \"count_type_id\": 3,\n",
    "        \"count_date_start\": [df.loc[1, 1]],\n",
    "        \"count_weather\": [df.loc[2, 1]],\n",
    "        \"h_station_date\": [df.loc[0, 1] + \"_\" + str(df.loc[1, 1])],\n",
    "        \"growth_rate_use\": [str(\"y\")],\n",
    "        \"count_interval\": [60],\n",
    "    }\n",
    "    header_temp = pd.DataFrame(header)\n",
    "    self.header_out_df = self.header_out_df.merge(header_temp, how=\"outer\")\n",
    "    # self.header_out_df = self.header_out_df.drop_duplicates()\n",
    "\n",
    "    data = df.loc[6:24, 0:5]\n",
    "    data.dropna(thresh=5)\n",
    "    data.rename(\n",
    "        columns={\n",
    "            0: \"count_hour\",\n",
    "            1: \"light\",\n",
    "            2: \"heavy\",\n",
    "            3: \"bus\",\n",
    "            4: \"taxi\",\n",
    "            5: \"total\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    data = data[data.count_hour.isin(DROP_IF) == False]\n",
    "\n",
    "    data[\"header_id\"] = header_temp.loc[0, \"header_id\"]\n",
    "\n",
    "    data[\"count_hour\"] = pd.to_datetime(\n",
    "        data[\"count_hour\"].str[:8], format=\"%H:%M:%S\"\n",
    "    ).dt.time\n",
    "\n",
    "    hour = data[\"count_hour\"].astype(str)\n",
    "    data[\"count_time\"] = header_temp.loc[0, \"count_date_start\"]\n",
    "    data[\"count_time\"] = pd.to_datetime(\n",
    "        data[\"count_time\"], format=\"%y/%m/%d\"\n",
    "    ) + pd.to_timedelta(hour)\n",
    "\n",
    "    data[\"header_date\"] = header_temp.loc[0, \"count_date_start\"]\n",
    "\n",
    "    data = check_if_calculated(data)\n",
    "    data_out_df = data_out_df.merge(data, how=\"outer\")\n",
    "    data_out_df = data_out_df.drop_duplicates()\n",
    "\n",
    "def etl_no_veryheavy( df, file) -> pd.DataFrame:\n",
    "\n",
    "    ## UNCOMMENT THIS WHEN WORKING WITH FILES\n",
    "    # xls = pd.ExcelFile(file)\n",
    "    # df = pd.read_excel(file, sheet_name=xls.sheet_names, header=None)\n",
    "    # for key, df in df.items():\n",
    "\n",
    "    try:\n",
    "\n",
    "        if pd.isnull(df.loc[23, 8]):\n",
    "            weather = \"sunny\"\n",
    "        else:\n",
    "            weather = df.loc[23, 8]\n",
    "\n",
    "        gid = str(uuid.uuid4())\n",
    "\n",
    "        header = {\n",
    "            \"header_id\": [gid],\n",
    "            \"document_url\": file,\n",
    "            \"counted_by\": [df.loc[16, 8]],\n",
    "            \"tc_station_name\": [str(df.loc[4, 8]) + str(df.loc[5, 8])],\n",
    "            \"count_type_id\": 3,\n",
    "            \"count_date_start\": [df.loc[2, 1]],\n",
    "            \"count_weather\": [weather],\n",
    "            \"h_station_date\": [gid],\n",
    "            # [\n",
    "            #     str(df.loc[4, 8]) + str(df.loc[5, 8]) + \"_\" + str(df.loc[2, 1])\n",
    "            # ],\n",
    "            \"growth_rate_use\": [str(\"Y\")],\n",
    "            \"count_interval\": [60],\n",
    "            \"latitude\": [df.loc[14, 8]],\n",
    "            \"longitude\": [df.loc[15, 8]],\n",
    "            \"kilometer_dist\": [df.loc[8, 8]],\n",
    "            \"road_link\": [df.loc[6, 8]],\n",
    "            \"type_of_count\": [df.loc[13, 8]],\n",
    "            \"description\": [\n",
    "                \"Between \" + str(df.loc[9, 8]) + \" and \" + str(df.loc[10, 8])\n",
    "            ],\n",
    "            \"count_duration_hours\": [df.loc[24, 8]],\n",
    "            \"no_days\": [df.loc[25, 8]],\n",
    "        }\n",
    "        header_temp = pd.DataFrame(header)\n",
    "\n",
    "        data = df.loc[4:29, 0:5]\n",
    "        data = data[(data[0] != \"Subtotal A\") & (data[0] != \"Subtotal B\")]\n",
    "        data = data.dropna(thresh=5)\n",
    "        data.rename(\n",
    "            columns={\n",
    "                0: \"count_hour\",\n",
    "                1: \"light\",\n",
    "                2: \"heavy\",\n",
    "                3: \"bus\",\n",
    "                4: \"taxi\",\n",
    "                5: \"total\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "        data[\"count_hour\"] = data[\"count_hour\"].str[:2]\n",
    "        data[\"h_station_date\"] = header_temp.loc[0, \"h_station_date\"]\n",
    "        data[\"tcname\"] = header_temp.loc[0, \"tc_station_name\"]\n",
    "\n",
    "        # data[\"count_hour\"] = pd.to_datetime(\n",
    "        #     data[\"count_hour\"].str[:8], format=\"%H\"\n",
    "        # ).dt.time\n",
    "\n",
    "        hour = data[\"count_hour\"].astype(str)\n",
    "        data[\"count_time\"] = header_temp.loc[0, \"count_date_start\"]\n",
    "        data[\"count_time\"] = pd.to_datetime(\n",
    "            data[\"count_time\"], format=\"%y/%m/%d\"\n",
    "        ) + pd.to_timedelta(hour)\n",
    "\n",
    "        data[\"header_date\"] = header_temp.loc[0, \"count_date_start\"]\n",
    "\n",
    "        new_datetime = (\n",
    "            header_temp.loc[0, \"count_date_start\"].strftime(\"%Y-%m-%d\") + \" \" + hour\n",
    "        )\n",
    "        data[\"count_hour\"] = pd.to_datetime(new_datetime)\n",
    "\n",
    "        data = check_if_calculated(data)\n",
    "\n",
    "        header_temp[\"count_duration_hours\"] = data[\"total\"].notnull().sum()\n",
    "        if header_temp[\"count_duration_hours\"].any() == 18:\n",
    "            data[\"count_type_id\"] = 4\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        header_out_df = header_out_df.merge(header_temp, how=\"outer\")\n",
    "        data_out_df = data_out_df.merge(data, how=\"outer\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"something wrong with the NO VERY HEAVY PROCESS\" + e)\n",
    "        with open(\n",
    "            os.path.expanduser(config.PROBLEM_FILES),\n",
    "            \"a\",\n",
    "            newline=\"\",\n",
    "        ) as f:\n",
    "            write = csv.writer(f)\n",
    "            write.writerows([[file]])\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(df, file, type, key):\n",
    "    if type == \"Basic Format\":\n",
    "        print(\"calculating using cumulative_etl\")\n",
    "        # cumulative_etl(df, file)\n",
    "    elif (type == \"Manual Traffic Counting Sheet\") and (\n",
    "        df.loc[3, 5] == \"Total\"\n",
    "    ):\n",
    "        print(\"calculating using etl_no_veryheavy\")\n",
    "        # etl_no_veryheavy(df, file)\n",
    "    elif (type == \"Manual Traffic Counting Sheet\") and (\n",
    "        df.loc[3, 6] == \"Total\"\n",
    "    ):\n",
    "        print(\"calculating using etl_template_form\")\n",
    "        etl_template_form(df, file)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_count_calc(df):\n",
    "    df2 = df\n",
    "    df[\"light\"] = df[\"light\"].diff().fillna(df[\"light\"])\n",
    "    df[\"heavy\"] = df[\"heavy\"].diff().fillna(df[\"heavy\"])\n",
    "    df[\"bus\"] = df[\"bus\"].diff().fillna(df[\"bus\"])\n",
    "    df[\"taxi\"] = df[\"taxi\"].diff().fillna(df[\"taxi\"])\n",
    "    df[\"total\"] = df[\"total\"].diff().fillna(df[\"total\"])\n",
    "    if (df.values < 0).any():\n",
    "        return df2\n",
    "    else:\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_calculated(data):\n",
    "    # print(\"checking if calculated\")\n",
    "    a = pd.Series(data[\"total\"])\n",
    "    amin = a.min()\n",
    "    avg = a.max() - amin\n",
    "    if avg == 0:\n",
    "        return data\n",
    "    else:\n",
    "        normalized_df = (a - amin) / avg\n",
    "        l = []\n",
    "        cnt = a.count()\n",
    "        for i in range(cnt):\n",
    "            if i == 0:\n",
    "                l.append(True)\n",
    "            elif (a.iloc[i] >= a.iloc[i-1]):\n",
    "                l.append(True)\n",
    "            else:\n",
    "                l.append(False)\n",
    "        if ((normalized_df.iloc[0] == 0) & (normalized_df.iloc[-1] == 1)) and all(element == True for element in l):\n",
    "            return hourly_count_calc(data)\n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def etl_template_form(df, file) -> pd.DataFrame:\n",
    "    # print(\"processing etl_template_form\")\n",
    "## UNCOMMENT THIS WHEN WORKING WITH FILES\n",
    "# xls = pd.ExcelFile(file)\n",
    "# df = pd.read_excel(file, sheet_name=xls.sheet_names, header=None)\n",
    "# for key, df in df.items():\n",
    "\n",
    "    if pd.isnull(df.loc[23, 9]):\n",
    "        weather = \"sunny\"\n",
    "    else:\n",
    "        weather = df.loc[23, 9]\n",
    "\n",
    "    gid = str(uuid.uuid4())\n",
    "\n",
    "    header = {\n",
    "        \"header_id\": [gid],\n",
    "        \"document_url\": file,\n",
    "        \"counted_by\": [df.loc[16, 9]],\n",
    "        \"tc_station_name\": [str(df.loc[4, 9]) + str(df.loc[5, 9])],\n",
    "        \"count_type_id\": 3,\n",
    "        \"count_date_start\": [df.loc[2, 1]],\n",
    "        \"count_weather\": [weather],\n",
    "        \"h_station_date\": [gid],\n",
    "        # [\n",
    "        #     str(df.loc[4, 8]) + str(df.loc[5, 8]) + \"_\" + str(df.loc[2, 1])\n",
    "        # ],\n",
    "        \"growth_rate_use\": [str(\"Y\")],\n",
    "        \"count_interval\": [60],\n",
    "        \"latitude\": [df.loc[14, 9]],\n",
    "        \"longitude\": [df.loc[15, 9]],\n",
    "        \"kilometer_dist\": [df.loc[8, 9]],\n",
    "        \"road_link\": [df.loc[6, 9]],\n",
    "        \"type_of_count\": [df.loc[13, 9]],\n",
    "        \"description\": [\n",
    "            \"Between \" + str(df.loc[9, 9]) + \" and \" + str(df.loc[10, 9])\n",
    "        ],\n",
    "        \"count_duration_hours\": [df.loc[24, 9]],\n",
    "        \"no_days\": [df.loc[25, 9]],\n",
    "    }\n",
    "    header_temp = pd.DataFrame(header)\n",
    "\n",
    "    data = df.loc[4:29, 0:6]\n",
    "    data = data[(data[0] != \"Subtotal A\") & (data[0] != \"Subtotal B\")]\n",
    "    data = data.dropna(thresh=5)\n",
    "    data.rename(\n",
    "        columns={\n",
    "            0: \"count_hour\",\n",
    "            1: \"light\",\n",
    "            2: \"heavy\",\n",
    "            3: \"veryheavy\",\n",
    "            4: \"bus\",\n",
    "            5: \"taxi\",\n",
    "            6: \"total\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    data[\"count_hour\"] = data[\"count_hour\"].str[:2]\n",
    "    data[\"h_station_date\"] = header_temp.loc[0, \"h_station_date\"]\n",
    "    data[\"tcname\"] = header_temp.loc[0, \"tc_station_name\"]\n",
    "\n",
    "    # data[\"count_hour\"] = pd.to_datetime(\n",
    "    #     data[\"count_hour\"].str[:8], format=\"%H\"\n",
    "    # ).dt.time\n",
    "\n",
    "    hour = data[\"count_hour\"].astype(str)\n",
    "    data[\"count_time\"] = header_temp.loc[0, \"count_date_start\"]\n",
    "    data[\"count_time\"] = pd.to_datetime(\n",
    "        data[\"count_time\"], format=\"%y/%m/%d\"\n",
    "    ) + pd.to_timedelta(hour)\n",
    "\n",
    "    data[\"header_date\"] = header_temp.loc[0, \"count_date_start\"]\n",
    "\n",
    "    new_datetime = (\n",
    "        header_temp.loc[0, \"count_date_start\"].strftime(\"%Y-%m-%d\") + \" \" + hour\n",
    "    )\n",
    "    data[\"count_hour\"] = pd.to_datetime(new_datetime)\n",
    "\n",
    "    data = check_if_calculated(data)\n",
    "\n",
    "    header_temp[\"count_duration_hours\"] = data[\"total\"].notnull().sum()\n",
    "    if header_temp[\"count_duration_hours\"].any() == 18:\n",
    "        data[\"count_type_id\"] = 4\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # header_out_df = header_out_df.merge(header_temp, how=\"outer\")\n",
    "    # data_out_df = data_out_df.merge(data, how=\"outer\")\n",
    "    return header_temp, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_out_df = pd.DataFrame(columns=config.HEADER)\n",
    "data_out_df = pd.DataFrame(columns=config.DATA)\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_excel(file, sheet_name=None, header=None,)\n",
    "    type = \"Manual Traffic Counting Sheet\"\n",
    "    for key, df in df.items():\n",
    "        l = get_completed_files()\n",
    "        if file + '-' + key in l:\n",
    "            pass\n",
    "        else:\n",
    "            try:\n",
    "\n",
    "                if type == \"Basic Format\":\n",
    "                    # print(\"calculating using cumulative_etl\")\n",
    "                    cumulative_etl(df, file)\n",
    "                elif (type == \"Manual Traffic Counting Sheet\") and (\n",
    "                    df.loc[3, 5] == \"Total\"\n",
    "                ):\n",
    "                    # print(\"calculating using etl_no_veryheavy\")\n",
    "                    etl_no_veryheavy(df, file)\n",
    "                elif (type == \"Manual Traffic Counting Sheet\") and (\n",
    "                    df.loc[3, 6] == \"Total\"\n",
    "                ):\n",
    "                    # print(\"calculating using etl_template_form\")\n",
    "                    header, data = etl_template_form(df, file)\n",
    "                    header_out_df = pd.concat([header_out_df,header], join='outer', axis=0, ignore_index=True)\n",
    "                    data_out_df = pd.concat([data_out_df,data], join='outer' , axis=0, ignore_index=True)\n",
    "                else:\n",
    "                    print(\"choosing something else\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(file + key)\n",
    "                print(df)\n",
    "\n",
    "header_out_df = header_out_df.dropna(axis=1, how = 'all')\n",
    "data_out_df = data_out_df.dropna(axis=1, how = 'all')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_out_df = header_out_df.drop_duplicates()\n",
    "data_out_df = data_out_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            count_hour light heavy veryheavy bus taxi total  \\\n",
      "0  2022-04-06 06:00:00     2     0         0   1    2     5   \n",
      "1  2022-04-06 07:00:00     3     0         0   0   50    53   \n",
      "2  2022-04-06 08:00:00     4     1         0   0   22    27   \n",
      "3  2022-04-06 09:00:00     9     0         0   0    1    10   \n",
      "4  2022-04-06 10:00:00     7     0         0   0    6    13   \n",
      "\n",
      "           header_date                     count_time  \\\n",
      "0  2022-04-06 00:00:00  2022-04-06 00:00:00.000000006   \n",
      "1  2022-04-06 00:00:00  2022-04-06 00:00:00.000000007   \n",
      "2  2022-04-06 00:00:00  2022-04-06 00:00:00.000000008   \n",
      "3  2022-04-06 00:00:00  2022-04-06 00:00:00.000000009   \n",
      "4  2022-04-06 00:00:00  2022-04-06 00:00:00.000000010   \n",
      "\n",
      "                         h_station_date tcname  \n",
      "0  4b01b81d-8eda-470a-adf3-857d31c8ace2   197A  \n",
      "1  4b01b81d-8eda-470a-adf3-857d31c8ace2   197A  \n",
      "2  4b01b81d-8eda-470a-adf3-857d31c8ace2   197A  \n",
      "3  4b01b81d-8eda-470a-adf3-857d31c8ace2   197A  \n",
      "4  4b01b81d-8eda-470a-adf3-857d31c8ace2   197A  \n"
     ]
    }
   ],
   "source": [
    "print(data_out_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_cols = list(pd.read_sql_query(\"\"\"SELECT * FROM trafc.manual_count_header limit 1;\"\"\", config.ENGINE).columns)\n",
    "\n",
    "data_cols = list(pd.read_sql_query(\"\"\"SELECT * FROM trafc.manual_count_data limit 1;\"\"\", config.ENGINE).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'document_url',\n",
       " 'counted_by',\n",
       " 'tc_station_name',\n",
       " 'count_type_id',\n",
       " 'count_date_start',\n",
       " 'count_weather',\n",
       " 'count_station_id',\n",
       " 'count_data_processed',\n",
       " 'processed_date',\n",
       " 'h_station_date',\n",
       " 'count_interval',\n",
       " 'growth_rate_use',\n",
       " 'count_duration_hours',\n",
       " 'direction',\n",
       " 'peakhr_ratio']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(header_out_df, data_out_df):\n",
    "\n",
    "    csv_export = True\n",
    "\n",
    "    header_out_df = header_out_df.drop(\n",
    "            columns=[\n",
    "                \"header_id\",\n",
    "                \"latitude\",\n",
    "                \"longitude\",\n",
    "                \"kilometer_dist\",\n",
    "                \"road_link\",\n",
    "                \"type_of_count\",\n",
    "                \"description\",\n",
    "                \"no_days\",\n",
    "            ], errors='ignore'\n",
    "        )\n",
    "\n",
    "    data_out_df = data_out_df.drop(\n",
    "        columns=[\"header_date\", \"count_time\", \"header_id\"], errors='ignore'\n",
    "    )\n",
    "    if csv_export == True:\n",
    "        try:\n",
    "            # EXPORT AS CSV\n",
    "            header_out_df.to_csv(config.HEADEROUT, mode=\"a\", index=False)\n",
    "            print(\"CSV HEADER DONE\")\n",
    "        except Exception:\n",
    "            print(\"\"\"something went wrong with the HEADER CSV EXPORT\"\"\")\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            data_out_df.to_csv(config.DATAOUT, mode=\"a\", index=False)\n",
    "            print(\"CSV DATA DONE\")\n",
    "        except Exception:\n",
    "            print(\"\"\"something went wrong with the DATA CSV EXPORT\"\"\")\n",
    "            pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV HEADER DONE\n",
      "CSV DATA DONE\n"
     ]
    }
   ],
   "source": [
    "export(header_out_df, data_out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"\"\"insert into trafc.manual_count_header (\n",
    "header_id,\n",
    "document_url,\n",
    "counted_by,\n",
    "tc_station_name,\n",
    "count_type_id,\n",
    "count_date_start,\n",
    "count_weather,\n",
    "h_station_date,\n",
    "growth_rate_use,\n",
    "count_interval,\n",
    "latitude,\n",
    "longitude,\n",
    "kilometer_dist,\n",
    "road_link,\n",
    "type_of_count,\n",
    "description,\n",
    "count_duration_hours,\n",
    "no_day)\n",
    "VALUES(\n",
    "    {row['header_id']},\n",
    "    {row['document_url']},\n",
    "    {row['counted_by']},\n",
    "    {row['tc_station_name']},\n",
    "    {row['count_type_id']},\n",
    "    {row['count_date_start']},\n",
    "    {row['count_weather']},\n",
    "    {row['h_station_date']},\n",
    "    {row['growth_rate_use']},\n",
    "    {row['count_interval']},\n",
    "    {row['latitude']},\n",
    "    {row['longitude']},\n",
    "    {row['kilometer_dist']},\n",
    "    {row['road_link']},\n",
    "    {row['type_of_count']},\n",
    "    {row['description']},\n",
    "    {row['count_duration_hours']},\n",
    "    {row['no_day']}\n",
    ")\n",
    "ON CONFLICT ON CONSTRAINT manual_count_header_un DO NOTHING\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_insert(row):\n",
    "    qry = f\"\"\"INSERT INTO trafc.manual_count_data\n",
    "    (manual_count_header_id, count_hour, light, heavy, veryheavy, bus, taxi, total, header_time, h_station_date, tcname)\n",
    "    VALUES(\n",
    "    {row['manual_count_header_id']}, \n",
    "    '{row['count_hour']}', \n",
    "    {row['light']}, \n",
    "    {row['heavy']}, \n",
    "    {row['veryheavy']}, \n",
    "    {row['bus']}, \n",
    "    {row['taxi']}, \n",
    "    {row['total']}, \n",
    "    '{row['header_time']}', \n",
    "    '{row['h_station_date']}', \n",
    "    '{row['tcname']}');\"\"\"\n",
    "    return qry\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f0240ba5a37eeffe7d380595dcf16248afe1beab2227011f0d1b1f050a1a57d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
